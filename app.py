"""
AI vs Human æ–‡ç« åˆ†é¡å·¥å…·
ä½¿ç”¨ Streamlit å»ºç«‹çš„ AI å…§å®¹åµæ¸¬å™¨
"""

import streamlit as st
import numpy as np
import pandas as pd
import re
import string
from collections import Counter
import plotly.express as px
import plotly.graph_objects as go
from sklearn.feature_extraction.text import TfidfVectorizer


def is_chinese_text(text):
    """åˆ¤æ–·æ–‡æœ¬æ˜¯å¦ä¸»è¦ç‚ºä¸­æ–‡"""
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    total_chars = len(re.sub(r'\s', '', text))
    return chinese_chars / total_chars > 0.3 if total_chars > 0 else False


def tokenize_text(text):
    """æ ¹æ“šæ–‡æœ¬èªè¨€é€²è¡Œåˆ†è©"""
    if is_chinese_text(text):
        # ä¸­æ–‡ï¼šæŒ‰å­—ç¬¦åˆ†è©ï¼ŒåŒæ™‚ä¿ç•™æ¨™é»ç¬¦è™Ÿåˆ†éš”
        # ç°¡å–®åˆ†è©ï¼šæ¯å€‹ä¸­æ–‡å­—ç‚ºä¸€å€‹ tokenï¼Œè‹±æ–‡å–®è©ä¿æŒå®Œæ•´
        tokens = []
        current_word = ""
        for char in text:
            if re.match(r'[\u4e00-\u9fff]', char):
                if current_word:
                    tokens.append(current_word)
                    current_word = ""
                tokens.append(char)
            elif re.match(r'[a-zA-Z0-9]', char):
                current_word += char
            else:
                if current_word:
                    tokens.append(current_word)
                    current_word = ""
        if current_word:
            tokens.append(current_word)
        return tokens
    else:
        # è‹±æ–‡ï¼šæŒ‰ç©ºæ ¼åˆ†è©
        return text.split()

# é é¢è¨­å®š
st.set_page_config(
    page_title="AI vs Human æ–‡ç« åˆ†é¡å™¨",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šç¾© CSS æ¨£å¼
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        color: #1E88E5;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.2rem;
        text-align: center;
        color: #666;
        margin-bottom: 2rem;
    }
    .result-box {
        padding: 20px;
        border-radius: 10px;
        margin: 10px 0;
    }
    .ai-result {
        background: linear-gradient(135deg, #ff6b6b, #ee5a5a);
        color: white;
    }
    .human-result {
        background: linear-gradient(135deg, #51cf66, #40c057);
        color: white;
    }
    .metric-card {
        background: #f8f9fa;
        padding: 15px;
        border-radius: 8px;
        text-align: center;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .stProgress > div > div > div > div {
        background-color: #1E88E5;
    }
</style>
""", unsafe_allow_html=True)


class TextFeatureExtractor:
    """æ–‡æœ¬ç‰¹å¾µæå–å™¨ - ä½¿ç”¨è‡ªå»ºç‰¹å¾µæ³•"""
    
    @staticmethod
    def extract_features(text):
        """æå–æ–‡æœ¬çš„å¤šç¶­ç‰¹å¾µ"""
        features = {}
        
        # åˆ¤æ–·èªè¨€
        is_chinese = is_chinese_text(text)
        features['is_chinese'] = is_chinese
        
        # åŸºæœ¬çµ±è¨ˆç‰¹å¾µ
        words = tokenize_text(text)
        
        # ä¸­æ–‡å¥å­åˆ†å‰²
        if is_chinese:
            sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿï¼Œ,;ï¼›]+', text)
        else:
            sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        features['word_count'] = len(words)
        features['char_count'] = len(text)
        features['sentence_count'] = len(sentences) if sentences else 1
        features['avg_word_length'] = np.mean([len(w) for w in words]) if words else 0
        features['avg_sentence_length'] = features['word_count'] / features['sentence_count']
        
        # è©å½™å¤šæ¨£æ€§
        unique_words = set(w.lower() for w in words)
        features['vocabulary_richness'] = len(unique_words) / len(words) if words else 0
        
        # æ¨™é»ç¬¦è™Ÿçµ±è¨ˆï¼ˆåŒ…å«ä¸­æ–‡æ¨™é»ï¼‰
        all_punctuation = string.punctuation + 'ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼šã€Œã€ã€ã€ã€ã€‘ï¼ˆï¼‰ã€Šã€‹ã€ˆã€‰'
        punctuation_count = sum(1 for c in text if c in all_punctuation)
        features['punctuation_ratio'] = punctuation_count / len(text) if text else 0
        
        # ç‰¹æ®Šå­—ç¬¦çµ±è¨ˆ
        features['comma_ratio'] = (text.count(',') + text.count('ï¼Œ')) / len(words) if words else 0
        features['semicolon_ratio'] = (text.count(';') + text.count('ï¼›')) / len(words) if words else 0
        
        # æ®µè½åˆ†æ
        paragraphs = text.split('\n\n')
        paragraphs = [p.strip() for p in paragraphs if p.strip()]
        features['paragraph_count'] = len(paragraphs)
        
        # å¥å­é•·åº¦è®Šç•°æ€§ï¼ˆé‡å°ä¸­æ–‡èª¿æ•´ï¼‰
        if is_chinese:
            sentence_lengths = [len(tokenize_text(s)) for s in sentences if s]
        else:
            sentence_lengths = [len(s.split()) for s in sentences if s]
        features['sentence_length_std'] = np.std(sentence_lengths) if len(sentence_lengths) > 1 else 0
        
        # é‡è¤‡è©æ¯”ä¾‹ï¼ˆAI å‚¾å‘é‡è¤‡ç‰¹å®šæ¨¡å¼ï¼‰
        word_freq = Counter(w.lower() for w in words)
        if words:
            most_common_freq = word_freq.most_common(1)[0][1] if word_freq else 0
            features['repetition_score'] = most_common_freq / len(words)
        else:
            features['repetition_score'] = 0
        
        # éæ¸¡è©æ¯”ä¾‹ï¼ˆAI å¸¸ç”¨éæ¸¡è©ï¼‰- æ“´å±•ä¸­æ–‡éæ¸¡è©
        transition_words_en = ['however', 'therefore', 'furthermore', 'moreover', 
                              'additionally', 'consequently', 'nevertheless', 'thus',
                              'hence', 'accordingly', 'meanwhile', 'subsequently']
        transition_words_zh = ['ç„¶è€Œ', 'å› æ­¤', 'æ­¤å¤–', 'è€Œä¸”', 'å¦å¤–', 'ç¸½ä¹‹', 'é¦–å…ˆ', 'å…¶æ¬¡',
                              'ç¶œä¸Šæ‰€è¿°', 'ç¸½è€Œè¨€ä¹‹', 'æ›å¥è©±èªª', 'ä¹Ÿå°±æ˜¯èªª', 'é€²ä¸€æ­¥', 'å…·é«”ä¾†èªª',
                              'ä¸€æ–¹é¢', 'å¦ä¸€æ–¹é¢', 'èˆ‡æ­¤åŒæ™‚', 'å€¼å¾—æ³¨æ„çš„æ˜¯', 'ä¸åƒ…å¦‚æ­¤']
        
        if is_chinese:
            # æª¢æŸ¥ä¸­æ–‡éæ¸¡è©ï¼ˆåœ¨åŸæ–‡ä¸­æœç´¢ï¼‰
            transition_count = sum(1 for tw in transition_words_zh if tw in text)
            features['transition_ratio'] = transition_count / features['sentence_count']
        else:
            transition_count = sum(1 for w in words if w.lower() in transition_words_en)
            features['transition_ratio'] = transition_count / len(words) if words else 0
        
        # å£èªåŒ–è¡¨é”åµæ¸¬ï¼ˆäººé¡ç‰¹å¾µï¼‰
        colloquial_zh = ['å•¦', 'å˜›', 'å‘¢', 'å§', 'å–”', 'å“¦', 'è€¶', 'æ¬¸', 'èª’', 'å—¯', 'å”‰',
                        'èªªå¯¦è©±', 'è€å¯¦èªª', 'å…¶å¯¦', 'åæ­£', 'ä¸é', 'è©±èªª', 'å°äº†',
                        'è¶…', 'å¾ˆ', 'è »', 'æŒº', 'é‚„ä¸éŒ¯', 'æ™®æ™®', 'é‚„å¥½', 'æœ‰é»']
        colloquial_en = ["i'm", "i've", "don't", "can't", "won't", "it's", "that's",
                        "gonna", "wanna", "gotta", "kinda", "sorta", "yeah", "nope",
                        "well", "anyway", "actually", "basically", "honestly"]
        
        if is_chinese:
            colloquial_count = sum(1 for cw in colloquial_zh if cw in text)
        else:
            colloquial_count = sum(1 for w in words if w.lower() in colloquial_en)
        features['colloquial_ratio'] = colloquial_count / features['sentence_count'] if features['sentence_count'] > 0 else 0
        
        # ç¬¬ä¸€äººç¨±ä½¿ç”¨ï¼ˆäººé¡ç‰¹å¾µï¼‰
        first_person_zh = ['æˆ‘', 'æˆ‘å€‘', 'æˆ‘çš„', 'æˆ‘è¦ºå¾—', 'æˆ‘èªç‚º', 'æˆ‘æƒ³']
        first_person_en = ['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']
        
        if is_chinese:
            first_person_count = sum(text.count(fp) for fp in first_person_zh)
        else:
            first_person_count = sum(1 for w in words if w.lower() in first_person_en)
        features['first_person_ratio'] = first_person_count / len(words) if words else 0
        
        # è¢«å‹•èªæ…‹æ¨™è¨˜è©ï¼ˆAI å‚¾å‘ä½¿ç”¨ï¼‰
        passive_markers = ['is', 'are', 'was', 'were', 'been', 'being', 'be']
        if not is_chinese:
            passive_count = sum(1 for w in words if w.lower() in passive_markers)
            features['passive_ratio'] = passive_count / len(words) if words else 0
        else:
            features['passive_ratio'] = 0
        
        # æ•¸å­—ä½¿ç”¨æ¯”ä¾‹
        digit_count = sum(1 for c in text if c.isdigit())
        features['digit_ratio'] = digit_count / len(text) if text else 0
        
        # å¤§å¯«å­—æ¯æ¯”ä¾‹
        upper_count = sum(1 for c in text if c.isupper())
        features['uppercase_ratio'] = upper_count / len(text) if text else 0
        
        # Burstinessï¼ˆè©å½™çªç™¼æ€§ï¼‰- AI æ–‡æœ¬é€šå¸¸æ›´å‡å‹»
        if len(words) > 10:
            word_positions = {}
            for i, w in enumerate(words):
                w_lower = w.lower()
                if w_lower not in word_positions:
                    word_positions[w_lower] = []
                word_positions[w_lower].append(i)
            
            bursts = []
            for positions in word_positions.values():
                if len(positions) > 1:
                    gaps = np.diff(positions)
                    bursts.append(np.std(gaps) if len(gaps) > 0 else 0)
            features['burstiness'] = np.mean(bursts) if bursts else 0
        else:
            features['burstiness'] = 0
        
        return features


class AIDetector:
    """AI åµæ¸¬å™¨ä¸»é¡åˆ¥"""
    
    def __init__(self):
        self.feature_extractor = TextFeatureExtractor()
        self.tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
        self.models = {}
        self._initialize_models()
    
    def _initialize_models(self):
        """åˆå§‹åŒ–é è¨“ç·´æ¨¡å‹ï¼ˆæ¨¡æ“¬ï¼‰"""
        # åœ¨å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œé€™è£¡æœƒè¼‰å…¥çœŸæ­£çš„é è¨“ç·´æ¨¡å‹
        # é€™è£¡ä½¿ç”¨å•Ÿç™¼å¼è¦å‰‡ä¾†æ¨¡æ“¬
        pass
    
    def analyze_text(self, text, method='ensemble'):
        """
        åˆ†ææ–‡æœ¬ä¸¦è¿”å› AI/Human æ©Ÿç‡
        
        Parameters:
        - text: è¼¸å…¥æ–‡æœ¬
        - method: åˆ†ææ–¹æ³• ('feature', 'statistical', 'ensemble')
        
        Returns:
        - dict: åŒ…å«åˆ†æçµæœ
        """
        if not text or len(text.strip()) < 50:
            return {
                'ai_probability': 0.5,
                'human_probability': 0.5,
                'confidence': 'low',
                'features': {},
                'warning': 'æ–‡æœ¬å¤ªçŸ­ï¼Œå»ºè­°è¼¸å…¥è‡³å°‘ 50 å€‹å­—ç¬¦ä»¥ç²å¾—æ›´æº–ç¢ºçš„çµæœ'
            }
        
        # æå–ç‰¹å¾µ
        features = self.feature_extractor.extract_features(text)
        
        # æ ¹æ“šä¸åŒæ–¹æ³•è¨ˆç®— AI æ©Ÿç‡
        if method == 'feature':
            ai_prob = self._feature_based_detection(features)
        elif method == 'statistical':
            ai_prob = self._statistical_detection(text, features)
        else:  # ensemble
            ai_prob = self._ensemble_detection(text, features)
        
        # ç¢ºå®šç½®ä¿¡åº¦
        if abs(ai_prob - 0.5) > 0.3:
            confidence = 'high'
        elif abs(ai_prob - 0.5) > 0.15:
            confidence = 'medium'
        else:
            confidence = 'low'
        
        return {
            'ai_probability': ai_prob,
            'human_probability': 1 - ai_prob,
            'confidence': confidence,
            'features': features,
            'method': method
        }
    
    def _feature_based_detection(self, features):
        """åŸºæ–¼ç‰¹å¾µçš„åµæ¸¬"""
        ai_score = 0.5
        is_chinese = features.get('is_chinese', False)
        
        # === äººé¡ç‰¹å¾µï¼ˆé™ä½ AI åˆ†æ•¸ï¼‰===
        
        # 1. å£èªåŒ–è¡¨é”ï¼ˆå¼·çƒˆçš„äººé¡ç‰¹å¾µï¼‰
        colloquial_ratio = features.get('colloquial_ratio', 0)
        if colloquial_ratio > 0.5:
            ai_score -= 0.25
        elif colloquial_ratio > 0.2:
            ai_score -= 0.15
        elif colloquial_ratio > 0:
            ai_score -= 0.1
        
        # 2. ç¬¬ä¸€äººç¨±ä½¿ç”¨ï¼ˆäººé¡ç‰¹å¾µï¼‰
        first_person_ratio = features.get('first_person_ratio', 0)
        if first_person_ratio > 0.05:
            ai_score -= 0.15
        elif first_person_ratio > 0.02:
            ai_score -= 0.08
        
        # 3. å¥å­é•·åº¦è®Šç•°æ€§é«˜ï¼ˆäººé¡å¯«ä½œæ›´ä¸è¦å‰‡ï¼‰
        sentence_std = features.get('sentence_length_std', 0)
        if is_chinese:
            if sentence_std > 8:
                ai_score -= 0.1
            elif sentence_std < 2:
                ai_score += 0.1
        else:
            if sentence_std > 15:
                ai_score -= 0.1
            elif sentence_std < 5:
                ai_score += 0.1
        
        # 4. çªç™¼æ€§é«˜ï¼ˆäººé¡ç‰¹å¾µï¼‰
        burstiness = features.get('burstiness', 0)
        if burstiness > 10:
            ai_score -= 0.1
        elif burstiness < 2:
            ai_score += 0.08
        
        # === AI ç‰¹å¾µï¼ˆå¢åŠ  AI åˆ†æ•¸ï¼‰===
        
        # 5. éæ¸¡è©ä½¿ç”¨ç‡é«˜ï¼ˆAI ç‰¹å¾µï¼‰
        transition_ratio = features.get('transition_ratio', 0)
        if is_chinese:
            if transition_ratio > 0.3:
                ai_score += 0.2
            elif transition_ratio > 0.1:
                ai_score += 0.1
        else:
            if transition_ratio > 0.05:
                ai_score += 0.15
            elif transition_ratio > 0.02:
                ai_score += 0.08
        
        # 6. è©å½™å¤šæ¨£æ€§ï¼ˆAI å‚¾å‘è¼ƒé«˜ï¼‰
        vocab_richness = features.get('vocabulary_richness', 0)
        if not is_chinese:  # è‹±æ–‡é©ç”¨
            if vocab_richness > 0.8:
                ai_score += 0.1
            elif vocab_richness < 0.5:
                ai_score -= 0.05
        
        # 7. è¢«å‹•èªæ…‹ï¼ˆAI å‚¾å‘ä½¿ç”¨ï¼Œåƒ…è‹±æ–‡ï¼‰
        if not is_chinese:
            passive_ratio = features.get('passive_ratio', 0)
            if passive_ratio > 0.1:
                ai_score += 0.1
        
        # 8. æ¨™é»ç¬¦è™Ÿä½¿ç”¨ï¼ˆä¸­æ–‡å£èªå¸¸ç”¨æ›´å¤šæ¨™é»ï¼‰
        punctuation_ratio = features.get('punctuation_ratio', 0)
        if is_chinese:
            if punctuation_ratio > 0.08:
                ai_score -= 0.05  # è¼ƒå¤šæ¨™é»å¯èƒ½æ˜¯å£èªåŒ–
        
        return np.clip(ai_score, 0.05, 0.95)
    
    def _statistical_detection(self, text, features):
        """åŸºæ–¼çµ±è¨ˆçš„åµæ¸¬"""
        ai_score = 0.5
        is_chinese = features.get('is_chinese', False)
        
        # ä½¿ç”¨æ­£ç¢ºçš„åˆ†è©æ–¹æ³•
        words = tokenize_text(text)
        words_lower = [w.lower() for w in words]
        
        # è¨ˆç®—è©é »åˆ†å¸ƒçš„ Zipf å®šå¾‹åé›¢åº¦
        word_freq = Counter(words_lower)
        freqs = sorted(word_freq.values(), reverse=True)
        
        if len(freqs) > 10:
            # ç†æƒ³ Zipf: freq(rank) âˆ 1/rank
            ranks = np.arange(1, min(len(freqs), 50) + 1)
            actual_freqs = np.array(freqs[:50]) if len(freqs) >= 50 else np.array(freqs)
            actual_freqs = actual_freqs[:len(ranks)]
            
            # è¨ˆç®—èˆ‡ç†æƒ³åˆ†å¸ƒçš„åé›¢
            ideal_freqs = actual_freqs[0] / ranks[:len(actual_freqs)]
            deviation = np.mean(np.abs(actual_freqs - ideal_freqs) / (ideal_freqs + 1))
            
            # AI æ–‡æœ¬é€šå¸¸åé›¢è¼ƒå°ï¼ˆæ›´ç¬¦åˆç†æƒ³åˆ†å¸ƒï¼‰
            if deviation < 0.3:
                ai_score += 0.1
            elif deviation > 0.6:
                ai_score -= 0.08
        
        # è€ƒæ…®å£èªåŒ–å’Œç¬¬ä¸€äººç¨±ï¼ˆçµ±è¨ˆå±¤é¢çš„äººé¡ç‰¹å¾µï¼‰
        colloquial_ratio = features.get('colloquial_ratio', 0)
        first_person_ratio = features.get('first_person_ratio', 0)
        
        if colloquial_ratio > 0.1 or first_person_ratio > 0.03:
            ai_score -= 0.15
        
        # N-gram é‡è¤‡æ¨¡å¼ï¼ˆåƒ…å°éä¸­æ–‡æœ‰æ•ˆï¼Œå› ç‚ºä¸­æ–‡å­—ç¬¦ç´šåˆ¥é‡è¤‡å¾ˆå¸¸è¦‹ï¼‰
        if not is_chinese and len(words) > 5:
            bigrams = [' '.join(words_lower[i:i+2]) for i in range(len(words_lower)-1)]
            trigrams = [' '.join(words_lower[i:i+3]) for i in range(len(words_lower)-2)]
            
            bigram_repetition = len(bigrams) - len(set(bigrams))
            trigram_repetition = len(trigrams) - len(set(trigrams))
            
            repetition_score = (bigram_repetition + trigram_repetition * 2) / len(words) if words else 0
            
            # AI å‚¾å‘æœ‰æ›´å¤šçš„çŸ­èªé‡è¤‡
            if repetition_score > 0.1:
                ai_score += 0.08
        
        return np.clip(ai_score, 0.05, 0.95)
    
    def _ensemble_detection(self, text, features):
        """æ•´åˆå¤šç¨®æ–¹æ³•çš„åµæ¸¬"""
        feature_score = self._feature_based_detection(features)
        statistical_score = self._statistical_detection(text, features)
        
        # åŠ æ¬Šå¹³å‡
        ensemble_score = 0.6 * feature_score + 0.4 * statistical_score
        
        return np.clip(ensemble_score, 0.05, 0.95)


def create_gauge_chart(ai_prob, human_prob):
    """å»ºç«‹å„€è¡¨æ¿åœ–è¡¨"""
    fig = go.Figure()
    
    # AI æ©Ÿç‡å„€è¡¨
    fig.add_trace(go.Indicator(
        mode="gauge+number",
        value=ai_prob * 100,
        title={'text': "AI ç”Ÿæˆæ©Ÿç‡", 'font': {'size': 20}},
        domain={'x': [0, 0.45], 'y': [0, 1]},
        gauge={
            'axis': {'range': [0, 100], 'tickwidth': 1},
            'bar': {'color': "#ff6b6b"},
            'bgcolor': "white",
            'borderwidth': 2,
            'bordercolor': "gray",
            'steps': [
                {'range': [0, 30], 'color': '#51cf66'},
                {'range': [30, 70], 'color': '#fcc419'},
                {'range': [70, 100], 'color': '#ff6b6b'}
            ],
            'threshold': {
                'line': {'color': "black", 'width': 4},
                'thickness': 0.75,
                'value': ai_prob * 100
            }
        }
    ))
    
    # Human æ©Ÿç‡å„€è¡¨
    fig.add_trace(go.Indicator(
        mode="gauge+number",
        value=human_prob * 100,
        title={'text': "äººé¡æ’°å¯«æ©Ÿç‡", 'font': {'size': 20}},
        domain={'x': [0.55, 1], 'y': [0, 1]},
        gauge={
            'axis': {'range': [0, 100], 'tickwidth': 1},
            'bar': {'color': "#51cf66"},
            'bgcolor': "white",
            'borderwidth': 2,
            'bordercolor': "gray",
            'steps': [
                {'range': [0, 30], 'color': '#ff6b6b'},
                {'range': [30, 70], 'color': '#fcc419'},
                {'range': [70, 100], 'color': '#51cf66'}
            ],
            'threshold': {
                'line': {'color': "black", 'width': 4},
                'thickness': 0.75,
                'value': human_prob * 100
            }
        }
    ))
    
    fig.update_layout(
        height=300,
        margin=dict(l=20, r=20, t=50, b=20),
        paper_bgcolor='rgba(0,0,0,0)',
        font={'family': "Arial"}
    )
    
    return fig


def create_feature_radar_chart(features):
    """å»ºç«‹ç‰¹å¾µé›·é”åœ–"""
    # é¸æ“‡é—œéµç‰¹å¾µä¸¦æ­£è¦åŒ–
    feature_names = [
        'è©å½™è±å¯Œåº¦', 'å¥å­è®Šç•°æ€§', 'éæ¸¡è©ä½¿ç”¨', 
        'çªç™¼æ€§', 'æ¨™é»ç¬¦è™Ÿ', 'é‡è¤‡åº¦'
    ]
    
    values = [
        min(features.get('vocabulary_richness', 0) * 100, 100),
        min(features.get('sentence_length_std', 0) * 5, 100),
        min(features.get('transition_ratio', 0) * 1000, 100),
        min(features.get('burstiness', 0) * 10, 100),
        min(features.get('punctuation_ratio', 0) * 1000, 100),
        min(features.get('repetition_score', 0) * 500, 100)
    ]
    
    fig = go.Figure()
    
    fig.add_trace(go.Scatterpolar(
        r=values + [values[0]],  # é–‰åˆåœ–å½¢
        theta=feature_names + [feature_names[0]],
        fill='toself',
        fillcolor='rgba(30, 136, 229, 0.3)',
        line=dict(color='#1E88E5', width=2),
        name='æ–‡æœ¬ç‰¹å¾µ'
    ))
    
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 100]
            )
        ),
        showlegend=False,
        title=dict(text='æ–‡æœ¬ç‰¹å¾µåˆ†æ', x=0.5),
        height=400,
        margin=dict(l=80, r=80, t=80, b=80)
    )
    
    return fig


def create_statistics_table(features):
    """å»ºç«‹çµ±è¨ˆè³‡è¨Šè¡¨æ ¼"""
    stats_data = {
        'æŒ‡æ¨™': [
            'ç¸½å­—æ•¸', 'ç¸½å­—ç¬¦æ•¸', 'å¥å­æ•¸é‡', 'æ®µè½æ•¸é‡',
            'å¹³å‡è©é•·', 'å¹³å‡å¥é•·', 'è©å½™è±å¯Œåº¦', 'æ¨™é»ç¬¦è™Ÿæ¯”ä¾‹'
        ],
        'æ•¸å€¼': [
            f"{features.get('word_count', 0):,}",
            f"{features.get('char_count', 0):,}",
            f"{features.get('sentence_count', 0):,}",
            f"{features.get('paragraph_count', 0):,}",
            f"{features.get('avg_word_length', 0):.2f}",
            f"{features.get('avg_sentence_length', 0):.2f}",
            f"{features.get('vocabulary_richness', 0):.2%}",
            f"{features.get('punctuation_ratio', 0):.2%}"
        ]
    }
    return pd.DataFrame(stats_data)


def main():
    """ä¸»ç¨‹å¼"""
    
    # æ¨™é¡Œ
    st.markdown('<h1 class="main-header">ğŸ¤– AI vs Human æ–‡ç« åˆ†é¡å™¨</h1>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’æŠ€è¡“åµæ¸¬æ–‡ç« æ˜¯å¦ç”± AI ç”Ÿæˆ</p>', unsafe_allow_html=True)
    
    # åˆå§‹åŒ–åµæ¸¬å™¨
    detector = AIDetector()
    
    # å´é‚Šæ¬„è¨­å®š
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        
        # é¸æ“‡åˆ†ææ–¹æ³•
        method = st.selectbox(
            "é¸æ“‡åˆ†ææ–¹æ³•",
            options=['ensemble', 'feature', 'statistical'],
            format_func=lambda x: {
                'ensemble': 'ğŸ”„ æ•´åˆåˆ†æ (æ¨è–¦)',
                'feature': 'ğŸ“Š ç‰¹å¾µåˆ†ææ³•',
                'statistical': 'ğŸ“ˆ çµ±è¨ˆåˆ†ææ³•'
            }.get(x, x),
            help="ensemble: çµåˆå¤šç¨®æ–¹æ³• | feature: åŸºæ–¼æ–‡æœ¬ç‰¹å¾µ | statistical: åŸºæ–¼çµ±è¨ˆåˆ†å¸ƒ"
        )
        
        st.markdown("---")
        
        st.header("ğŸ“– ä½¿ç”¨èªªæ˜")
        st.markdown("""
        1. åœ¨æ–‡æœ¬æ¡†ä¸­è²¼ä¸Šè¦åˆ†æçš„æ–‡ç« 
        2. é»æ“Šã€Œé–‹å§‹åˆ†æã€æŒ‰éˆ•
        3. æŸ¥çœ‹ AI/Human æ©Ÿç‡çµæœ
        4. æª¢è¦–è©³ç´°ç‰¹å¾µåˆ†æ
        
        **æç¤ºï¼š** å»ºè­°è¼¸å…¥è‡³å°‘ 100 å­—ä»¥ç²å¾—æ›´æº–ç¢ºçš„çµæœ
        """)
        
        st.markdown("---")
        
        st.header("â„¹ï¸ é—œæ–¼")
        st.markdown("""
        æ­¤å·¥å…·ä½¿ç”¨å¤šç¨®è‡ªç„¶èªè¨€è™•ç†æŠ€è¡“ä¾†åˆ¤æ–·æ–‡æœ¬æ˜¯å¦ç”± AI ç”Ÿæˆï¼š
        
        - **è©å½™åˆ†æ**: æª¢æ¸¬è©å½™å¤šæ¨£æ€§èˆ‡ä½¿ç”¨æ¨¡å¼
        - **å¥æ³•åˆ†æ**: åˆ†æå¥å­çµæ§‹èˆ‡é•·åº¦è®ŠåŒ–
        - **çµ±è¨ˆåˆ†æ**: è©•ä¼°æ–‡æœ¬çš„çµ±è¨ˆç‰¹å¾µ
        - **æ¨¡å¼è­˜åˆ¥**: è­˜åˆ¥ AI ç”Ÿæˆæ–‡æœ¬çš„å…¸å‹æ¨¡å¼
        """)
    
    # ä¸»è¦å…§å®¹å€
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ“ è¼¸å…¥æ–‡æœ¬")
        
        # ç¯„ä¾‹æ–‡æœ¬
        sample_texts = {
            "è«‹é¸æ“‡...": "",
            "AI ç”Ÿæˆç¯„ä¾‹": """Artificial intelligence has revolutionized the way we interact with technology. Furthermore, it has transformed various industries, including healthcare, finance, and education. The implementation of machine learning algorithms has enabled systems to learn from data and make intelligent decisions. Moreover, natural language processing has made it possible for computers to understand and generate human language with remarkable accuracy. Consequently, businesses are increasingly adopting AI solutions to improve efficiency and productivity. Additionally, the continuous advancement in deep learning techniques has opened new possibilities for innovation and discovery.""",
            "äººé¡æ’°å¯«ç¯„ä¾‹": """æ˜¨å¤©æˆ‘å»äº†ä¸€å®¶æ–°é–‹çš„å’–å•¡åº—ï¼Œèªªå¯¦è©±ï¼Œæœ‰é»å¤±æœ›ã€‚åº—é¢è£æ½¢é‚„ä¸éŒ¯å•¦ï¼Œå¾ˆæœ‰æ–‡é’é¢¨ï¼Œä½†å’–å•¡å‘³é“æ™®æ™®ã€‚æˆ‘é»äº†ä¸€æ¯æ‹¿éµï¼Œçµæœç­‰äº†å¿«äºŒååˆ†é˜æ‰é€ä¾†ï¼Œè€Œä¸”æº«åº¦ä¸å¤ ç†±ã€‚ä¸éä»–å€‘çš„ç”œé»å€’æ˜¯æŒºå¥½åƒçš„ï¼Œé‚£å€‹ææ‹‰ç±³è˜‡å…¥å£å³åŒ–ã€‚ä¸‹æ¬¡å¯èƒ½æœƒå†å»è©¦è©¦å…¶ä»–å“é …ï¼Œä½†ç´”å–å’–å•¡çš„è©±ï¼Œæˆ‘é‚„æ˜¯æœƒé¸æ“‡è€åº—ã€‚"""
        }
        
        selected_sample = st.selectbox("é¸æ“‡ç¯„ä¾‹æ–‡æœ¬", list(sample_texts.keys()))
        
        text_input = st.text_area(
            "è²¼ä¸Šæ‚¨è¦åˆ†æçš„æ–‡ç« ",
            value=sample_texts[selected_sample],
            height=300,
            placeholder="è«‹åœ¨æ­¤è¼¸å…¥æˆ–è²¼ä¸Šè¦åˆ†æçš„æ–‡æœ¬...\n\nå»ºè­°è‡³å°‘ 100 å­—ä»¥ç²å¾—æ›´æº–ç¢ºçš„çµæœã€‚"
        )
        
        analyze_button = st.button("ğŸ” é–‹å§‹åˆ†æ", type="primary", use_container_width=True)
    
    with col2:
        st.subheader("ğŸ“Š å¿«é€Ÿçµ±è¨ˆ")
        if text_input:
            word_count = len(text_input.split())
            char_count = len(text_input)
            sentence_count = len(re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', text_input))
            
            st.metric("å­—æ•¸", f"{word_count:,}")
            st.metric("å­—ç¬¦æ•¸", f"{char_count:,}")
            st.metric("å¥å­æ•¸", f"{sentence_count:,}")
            
            # æ–‡æœ¬é•·åº¦è­¦å‘Š
            if char_count < 100:
                st.warning("âš ï¸ æ–‡æœ¬è¼ƒçŸ­ï¼Œçµæœå¯èƒ½ä¸å¤ æº–ç¢º")
            elif char_count > 5000:
                st.info("â„¹ï¸ è¼ƒé•·çš„æ–‡æœ¬é€šå¸¸èƒ½ç²å¾—æ›´æº–ç¢ºçš„çµæœ")
        else:
            st.info("è«‹è¼¸å…¥æ–‡æœ¬ä»¥æŸ¥çœ‹çµ±è¨ˆè³‡è¨Š")
    
    # åˆ†æçµæœ
    if analyze_button and text_input:
        with st.spinner("æ­£åœ¨åˆ†æä¸­..."):
            result = detector.analyze_text(text_input, method=method)
        
        st.markdown("---")
        st.subheader("ğŸ“ˆ åˆ†æçµæœ")
        
        # é¡¯ç¤ºè­¦å‘Šï¼ˆå¦‚æœæœ‰ï¼‰
        if 'warning' in result:
            st.warning(result['warning'])
        
        # å„€è¡¨æ¿åœ–è¡¨
        gauge_fig = create_gauge_chart(result['ai_probability'], result['human_probability'])
        st.plotly_chart(gauge_fig, use_container_width=True)
        
        # çµæœæ‘˜è¦
        col_result1, col_result2, col_result3 = st.columns(3)
        
        with col_result1:
            ai_class = "ai-result" if result['ai_probability'] > 0.5 else ""
            st.markdown(f"""
            <div class="metric-card">
                <h3>ğŸ¤– AI æ©Ÿç‡</h3>
                <h1 style="color: #ff6b6b;">{result['ai_probability']:.1%}</h1>
            </div>
            """, unsafe_allow_html=True)
        
        with col_result2:
            st.markdown(f"""
            <div class="metric-card">
                <h3>ğŸ‘¤ äººé¡æ©Ÿç‡</h3>
                <h1 style="color: #51cf66;">{result['human_probability']:.1%}</h1>
            </div>
            """, unsafe_allow_html=True)
        
        with col_result3:
            confidence_color = {'high': '#51cf66', 'medium': '#fcc419', 'low': '#ff6b6b'}
            confidence_text = {'high': 'é«˜', 'medium': 'ä¸­', 'low': 'ä½'}
            st.markdown(f"""
            <div class="metric-card">
                <h3>ğŸ“Š ç½®ä¿¡åº¦</h3>
                <h1 style="color: {confidence_color[result['confidence']]};">{confidence_text[result['confidence']]}</h1>
            </div>
            """, unsafe_allow_html=True)
        
        # åˆ¤æ–·çµæœ
        st.markdown("---")
        if result['ai_probability'] > 0.7:
            st.error("ğŸ¤– **åˆ¤æ–·çµæœ**: æ­¤æ–‡æœ¬å¾ˆå¯èƒ½ç”± AI ç”Ÿæˆ")
        elif result['ai_probability'] > 0.5:
            st.warning("âš ï¸ **åˆ¤æ–·çµæœ**: æ­¤æ–‡æœ¬å¯èƒ½åŒ…å« AI ç”Ÿæˆçš„å…§å®¹")
        elif result['ai_probability'] > 0.3:
            st.info("â„¹ï¸ **åˆ¤æ–·çµæœ**: æ­¤æ–‡æœ¬å¯èƒ½å¤§éƒ¨åˆ†ç”±äººé¡æ’°å¯«")
        else:
            st.success("ğŸ‘¤ **åˆ¤æ–·çµæœ**: æ­¤æ–‡æœ¬å¾ˆå¯èƒ½ç”±äººé¡æ’°å¯«")
        
        # è©³ç´°åˆ†æ
        st.markdown("---")
        st.subheader("ğŸ”¬ è©³ç´°åˆ†æ")
        
        tab1, tab2, tab3 = st.tabs(["ğŸ“Š ç‰¹å¾µé›·é”åœ–", "ğŸ“‹ çµ±è¨ˆè³‡è¨Š", "ğŸ” ç‰¹å¾µè©³æƒ…"])
        
        with tab1:
            radar_fig = create_feature_radar_chart(result['features'])
            st.plotly_chart(radar_fig, use_container_width=True)
            
            st.markdown("""
            **åœ–è¡¨èªªæ˜ï¼š**
            - **è©å½™è±å¯Œåº¦**: ä½¿ç”¨ç¨ç‰¹è©å½™çš„æ¯”ä¾‹
            - **å¥å­è®Šç•°æ€§**: å¥å­é•·åº¦çš„è®ŠåŒ–ç¨‹åº¦
            - **éæ¸¡è©ä½¿ç”¨**: ä½¿ç”¨é€£æ¥è©å’Œéæ¸¡è©çš„é »ç‡
            - **çªç™¼æ€§**: è©å½™å‡ºç¾çš„ä¸è¦å¾‹ç¨‹åº¦
            - **æ¨™é»ç¬¦è™Ÿ**: æ¨™é»ç¬¦è™Ÿä½¿ç”¨æ¯”ä¾‹
            - **é‡è¤‡åº¦**: è©å½™é‡è¤‡å‡ºç¾çš„ç¨‹åº¦
            """)
        
        with tab2:
            stats_df = create_statistics_table(result['features'])
            st.dataframe(stats_df, use_container_width=True, hide_index=True)
            
            # åˆ†å¸ƒåœ–
            if result['features'].get('word_count', 0) > 0:
                words = text_input.split()
                word_lengths = [len(w) for w in words]
                
                fig_dist = px.histogram(
                    x=word_lengths, 
                    nbins=20,
                    title="è©é•·åˆ†å¸ƒ",
                    labels={'x': 'è©é•·', 'count': 'é »ç‡'}
                )
                fig_dist.update_layout(
                    showlegend=False,
                    height=300
                )
                st.plotly_chart(fig_dist, use_container_width=True)
        
        with tab3:
            st.markdown("#### æ‰€æœ‰æå–çš„ç‰¹å¾µå€¼")
            
            feature_df = pd.DataFrame([
                {'ç‰¹å¾µåç¨±': k, 'æ•¸å€¼': f"{v:.4f}" if isinstance(v, float) else str(v)}
                for k, v in result['features'].items()
            ])
            st.dataframe(feature_df, use_container_width=True, hide_index=True)
            
            st.markdown("""
            **AI æ–‡æœ¬çš„å…¸å‹ç‰¹å¾µï¼š**
            - âœ“ è¼ƒé«˜çš„è©å½™å¤šæ¨£æ€§
            - âœ“ è¼ƒä¸€è‡´çš„å¥å­é•·åº¦ï¼ˆä½è®Šç•°æ€§ï¼‰
            - âœ“ è¼ƒå¤šä½¿ç”¨éæ¸¡è©å’Œé€£æ¥è©
            - âœ“ è¼ƒä½çš„çªç™¼æ€§ï¼ˆè©å½™åˆ†å¸ƒå‡å‹»ï¼‰
            - âœ“ çµæ§‹åŒ–çš„æ®µè½å®‰æ’
            """)
    
    # é å°¾
    st.markdown("---")
    st.markdown("""
    <div style="text-align: center; color: #666; padding: 20px;">
        <p>ğŸ’¡ æ­¤å·¥å…·åƒ…ä¾›åƒè€ƒï¼Œçµæœä¸èƒ½ä½œç‚ºçµ•å°åˆ¤æ–·ä¾æ“š</p>
        <p>Made with â¤ï¸ using Streamlit | AIOT HW5</p>
    </div>
    """, unsafe_allow_html=True)


if __name__ == "__main__":
    main()
